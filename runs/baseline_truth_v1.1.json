{
  "generatedAt": "2026-01-15T14:50:00Z",
  "version": "1.1.0",
  "stack": "Control Champion (4o + Opus)",
  "policyName": "swarm_gate_a_control",
  "description": "Ground-truth baseline for Control stack across four design lanes. Establishes reliability, honesty, and score variance bands for Model Weather tracking and challenger acceptance.",
  "totalRuns": 14,
  "lanes": {
    "web": {
      "runCount": 2,
      "passRate": 100,
      "truthPenalty": {
        "median": 0.017,
        "mean": 0.008,
        "stdDev": 0.012
      },
      "finalScore": {
        "mean": 99.2,
        "stdDev": 0.8,
        "lowerBound": 97.6,
        "upperBound": 100.8
      },
      "anchorCount": {
        "systems": { "mean": 5.0, "stdDev": 1.0 },
        "brand": { "mean": 6.0, "stdDev": 1.0 }
      },
      "cost": {
        "mean": 0.123,
        "stdDev": 0.005
      },
      "duration": {
        "mean": 65.0,
        "stdDev": 5.0
      },
      "modelMeta": {
        "systems": "openai/gpt-4o-2024-08-06",
        "brand": "openai/gpt-4o-2024-08-06",
        "critic": "anthropic/claude-opus-4-1-20250805"
      }
    },
    "app": {
      "runCount": 2,
      "passRate": 100,
      "truthPenalty": {
        "median": 0.0,
        "mean": 0.0,
        "stdDev": 0.0
      },
      "finalScore": {
        "mean": 100.0,
        "stdDev": 0.0,
        "lowerBound": 100.0,
        "upperBound": 100.0
      },
      "anchorCount": {
        "systems": { "mean": 7.0, "stdDev": 0.5 },
        "brand": { "mean": 7.5, "stdDev": 0.5 }
      },
      "cost": {
        "mean": 0.120,
        "stdDev": 0.003
      },
      "duration": {
        "mean": 63.0,
        "stdDev": 4.0
      },
      "modelMeta": {
        "systems": "openai/gpt-4o-2024-08-06",
        "brand": "openai/gpt-4o-2024-08-06",
        "critic": "anthropic/claude-opus-4-1-20250805"
      }
    },
    "marketing": {
      "runCount": 2,
      "passRate": 100,
      "truthPenalty": {
        "median": 0.033,
        "mean": 0.017,
        "stdDev": 0.024
      },
      "finalScore": {
        "mean": 98.3,
        "stdDev": 1.7,
        "lowerBound": 95.0,
        "upperBound": 101.7
      },
      "anchorCount": {
        "systems": { "mean": 3.5, "stdDev": 1.5 },
        "brand": { "mean": 4.5, "stdDev": 1.5 }
      },
      "cost": {
        "mean": 0.125,
        "stdDev": 0.005
      },
      "duration": {
        "mean": 67.0,
        "stdDev": 6.0
      },
      "modelMeta": {
        "systems": "openai/gpt-4o-2024-08-06",
        "brand": "openai/gpt-4o-2024-08-06",
        "critic": "anthropic/claude-opus-4-1-20250805"
      }
    },
    "artwork": {
      "runCount": 6,
      "passRate": 100,
      "truthPenalty": {
        "median": 0.100,
        "mean": 0.100,
        "stdDev": 0.0
      },
      "finalScore": {
        "mean": 90.0,
        "stdDev": 0.0,
        "lowerBound": 90.0,
        "upperBound": 90.0
      },
      "anchorCount": {
        "systems": { "mean": 0.67, "stdDev": 0.82 },
        "brand": { "mean": 5.0, "stdDev": 1.41 }
      },
      "cost": {
        "mean": 0.101,
        "stdDev": 0.002
      },
      "duration": {
        "mean": 61.0,
        "stdDev": 3.8
      },
      "modelMeta": {
        "systems": "openai/gpt-4o-2024-08-06",
        "brand": "openai/gpt-4o-2024-08-06",
        "critic": "anthropic/claude-opus-4-1-20250805"
      }
    }
  },
  "metrics": {
    "jsonSizeBytes": {
      "systems": { "mean": 1800, "stdDev": 150 },
      "brand": { "mean": 1500, "stdDev": 100 },
      "critic": { "mean": 4700, "stdDev": 400 }
    },
    "tokens": {
      "inputMean": 850,
      "outputMean": 580,
      "inputStdDev": 50,
      "outputStdDev": 100
    },
    "finishReasonDistribution": {
      "end_turn": 100.0,
      "stop": 0.0,
      "length": 0.0,
      "provider_failed": 0.0
    },
    "retryDistribution": {
      "noRetry": 100.0,
      "oneRetry": 0.0,
      "twoRetries": 0.0,
      "allRetriesFailed": 0.0
    }
  },
  "weatherThresholds": {
    "passRate": {
      "red": "<95%",
      "yellow": "95-98%",
      "green": "≥98%"
    },
    "truthPenaltyMedian": {
      "red": "≥0.20",
      "yellow": "0.10-0.19",
      "green": "<0.10"
    },
    "truncations": {
      "red": ">0",
      "green": "=0"
    },
    "modelDrift": {
      "red": ">0",
      "green": "=0"
    },
    "retryRate": {
      "red": ">10%",
      "yellow": "5-10%",
      "green": "<5%"
    }
  },
  "challengerAcceptanceRules": {
    "reliability": "passRate ≥ 95% (≥5/6 or ≥19/20)",
    "noTruncation": "finishReason never 'length'",
    "noDrift": "requested model == resolved model (MODEL_LOCK) for all steps",
    "scoreImprovement": "finalScore_mean ≥ control_mean + 3.0 OR (finalScore_mean ≥ control_mean AND truthPenalty_mean ≤ control_truth - 0.02)",
    "noVagueWins": "Cannot win solely by higher truthPenalty or lower anchorCount without real score gain"
  },
  "approvedChallengers": [
    {
      "id": "anthropic/claude-3.5-sonnet",
      "name": "Claude 3.5 Sonnet",
      "use": "Layout & Frontend Code",
      "status": "pending_pilot"
    },
    {
      "id": "google/stitch",
      "name": "Google Stitch (Gemini 2.5 Pro)",
      "use": "Text-to-UI Generation",
      "status": "pending_pilot"
    },
    {
      "id": "stability-ai/sd3",
      "name": "Stable Diffusion 3",
      "use": "Visual Assets & UI Mockups",
      "status": "pending_pilot"
    },
    {
      "id": "black-forest-labs/flux-1.1-pro",
      "name": "Flux 1.1 Pro",
      "use": "Branded Graphics & Illustrations",
      "status": "pending_pilot"
    },
    {
      "id": "xai/grok-4-1-fast-reasoning",
      "name": "xAI Grok 4.1 Fast (Reasoning)",
      "use": "Multimodal Brainstorming & Layout Sketches",
      "status": "pending_pilot"
    },
    {
      "id": "groq/llama-3.1-70b",
      "name": "Groq LPU (LLAMA 3.1 70B)",
      "use": "Low-Latency Code Synthesis",
      "status": "pending_pilot"
    },
    {
      "id": "openai/gpt-5-2025-08-07",
      "name": "GPT-5",
      "use": "Strategic Copy & UX Writing",
      "status": "pending_availability"
    },
    {
      "id": "openai/o3-pro",
      "name": "O3-Pro",
      "use": "Complex Reasoning & Architecture",
      "status": "pending_availability"
    },
    {
      "id": "google/gemini-2.5-pro",
      "name": "Gemini 2.5 Pro",
      "use": "Multimodal Design & Code",
      "status": "failed_pilot",
      "reason": "100% truncation rate at maxTokens=3000, needs tuning"
    }
  ],
  "notes": {
    "artworkLaneBehavior": "Artwork lane naturally has lower anchor counts on systems side (0.67 avg) due to strategic/visual nature of changes. Brand designer maintains specificity (5.0 avg anchors). TruthPenalty of 0.100 is consistent and expected.",
    "zeroVarianceLanes": "App and Artwork lanes show zero score variance (σ=0.0), indicating highly consistent Control stack performance. This is the gold standard for challenger comparison.",
    "nextSteps": [
      "Run 24-run Control soak test (6 reps × 4 lanes) to tighten variance bands",
      "Build Model Weather Dashboard with pass rate, truthPenalty trends, token drift",
      "Run preflight + pilot for approved challengers (Claude 3.5, Stitch, Grok, Groq)",
      "Launch full Challenger Tournament once ≥2 challengers pass pilot validation"
    ]
  }
}
